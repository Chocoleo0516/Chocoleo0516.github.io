<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>DDPM 正向过程的通项公式 | Chocoleo的博客</title>
  
  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">

  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      svg: {
        fontCache: 'global'
      }
    };
  </script>
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+SC&display=swap" rel="stylesheet">

</head>
<body>
  <header>
    <div class="container">
      <h1><a href="/">Chocoleo的博客</a></h1>
      <nav>
      
        
        <a href="/">首页</a>
      
        
        <a href="/about.html">关于</a>
      
      </nav>
      <p>这里是我的个人博客，记录学习和思考的过程。</p>
    </div>
  </header>
  <main>
    <div class="container">
      <article>
  <h2>DDPM 正向过程的通项公式</h2>
  <p class="post-meta">
    <time datetime="2025-09-12T21:08:47+08:00">2025年09月12日</time>
  </p>

  <div class="post-content">
    <p>Denoising Diffusion Probabilistic Models (DDPM) 的核心是两个过程：正向加噪和逆向采样过程，本文我们将聚焦正向加噪过程：</p>

<p>论文Background章节的的第二个公式，描述了一个<strong>逐步演化</strong>的过程：</p>

\[q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})\]

<p>它又可以写成：</p>

\[x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_{t-1}\]

<p>它告诉我们，任意时刻 \(t\) 的图像 \(x_t\)，是由其前一时刻的图像 \(x_{t-1}\) 加上少量噪声得到的。</p>

<p>这是一个典型的马尔可夫过程。</p>

<p>而在正向过程的伪代码中，论文则给出了一个<strong>“一步到位”</strong>的计算方法：</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\]

<p>这个公式表明，我们可以直接从最原始的清晰图像 \(x_0\)，一步跳到任意时刻 \(t\) 的加噪状态 \(x_t\)。</p>

<p>这也许会让你感到疑惑，<strong>既然扩散过程的理论基础是逐步演化的，为什么我们可以从 $ x_0 $ 直接推出 \(x_t\) ？上面两个公式之间究竟是什么关系？</strong></p>

<p>本文将围绕这个核心问题，深入剖析后一个公式是如何从第一个公式推导而来的，它背后深刻的物理意义，并给出详尽的数学证明。</p>

<h2 id="1-为何需要一步到位">1. 为何需要“一步到位”？</h2>
<p>虽然理论上，扩散模型的正向过程（也就是加噪的过程，即训练阶段）是一步一步来的，但是在实际训练时，我们实际上是取某一个时间步的图像进行训练。这个区别关键在于区分理论基础与实践需求。</p>

<ul>
  <li>
    <p><strong>分步定义是理论基石</strong>：正向过程的本质是一个<strong>马尔可夫链</strong>，即当前状态 $ x_t $ 只依赖于前一个状态 $ x_{t-1} $。这个分步定义 $ q(x_t \mid x_{t-1}) $ 是整个扩散模型的理论核心。它保证了过程的平滑性和数学上的可逆性，这为后续的反向去噪过程（即学习预测 $ p(x_{t-1} \mid x_t) $）提供了坚实的理论依据。</p>
  </li>
  <li>
    <p><strong>直接计算是实践需求</strong>：在训练神经网络时，我们的目标是让模型学会从任意一张加噪图像 $ x_t $ 中恢复信息（例如，预测所添加的噪声 \(\epsilon\)）。为了高效训练，我们需要快速生成海量的训练数据。如果每次生成 $ x_t $ 都需要从 $ x_0 $ 开始迭代 $ t $ 次，当 $ t $ 很大时，这个过程将极其耗时，使得模型训练变得不切实际。</p>
  </li>
</ul>

<p>因此，这个“一步到位”的公式正是连接理论与实践的桥梁。它是一个从分步定义中推导出的解析解，让我们能够在常数时间内完成采样，极大地提高了训练效率。</p>

<h2 id="2-公式推导">2. 公式推导</h2>

<p>“一步到位”公式的推导利用了高斯分布的一个性质：<strong>两个独立高斯分布的和仍然是一个高斯分布</strong>。具体来说，如果 \(Z_1 \sim \mathcal{N}(\mu_1, \sigma_1^2\mathbf{I})\) 和 \(Z_2 \sim \mathcal{N}(\mu_2, \sigma_2^2\mathbf{I})\) 是独立的，那么它们的和 \(Z_1 + Z_2 \sim \mathcal{N}(\mu_1+\mu_2, (\sigma_1^2 + \sigma_2^2)\mathbf{I})\)。</p>

<p>从单步加噪的公式出发，我们定义 \(\alpha_t = 1 - \beta_t\)，单步过程写作：</p>

\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}, \quad \text{其中 } \epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})\]

<p>将 \(x_{t-1}\) 的表达式 \(x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}\) 代入上式：</p>

\[\begin{aligned}
x_t &amp;= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
&amp;= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \underbrace{\sqrt{\alpha_t(1 - \alpha_{t-1})} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}}_{\text{两个独立高斯噪声的和}}
\end{aligned}\]

<p>根据高斯分布的可加性，后面两个噪声项可以合并成一个单一的高斯噪声。不断地重复上述过程直到回溯到 \(x_0\)，最终就能得到我们的目标公式。</p>

<h2 id="3-意义">3. 意义</h2>
<p>这个公式揭示了正向过程的本质：</p>

<ol>
  <li>
    <p>任意时刻的加噪图像 \(x_t\) 都可以看作是<strong>原始图像 \(x_0\)（信号）</strong>和<strong>标准高斯噪声 \(\epsilon\)（噪声）</strong>的一个线性组合。</p>
  </li>
  <li>系数 \(\bar{\alpha}_t = \prod_{i=1}^{t} (1 - \beta_i)\) 是一个随 \(t\) 增大而单调递减的数值。它就像一个“进度条”，控制着信号与噪声的平衡：
    <ul>
      <li><strong>信号强度</strong>由 \(\sqrt{\bar{\alpha}_t}\) 控制。当 \(t\) 很小时，\(\bar{\alpha}_t\) 接近1，\(x_t\) 主要由 \(x_0\) 决定。</li>
      <li><strong>噪声强度</strong>由 \(\sqrt{1 - \bar{\alpha}_t}\) 控制。当 \(t\) 增大时，\(\bar{\alpha}_t\) 趋向0，\(x_t\) 中的噪声成分越来越重。</li>
      <li>当 \(t=T\) 时，\(\bar{\alpha}_T \approx 0\)，此时 \(x_T \approx \epsilon\)。这意味着无论原始图像是什么，最终都会变成一个纯粹的标准高斯噪声。</li>
    </ul>
  </li>
  <li><strong>训练的基石</strong>：这个公式使得模型的训练目标变得非常明确。给定 \(x_0\) 和 \(t\)，我们可以通过这个公式生成 \(x_t\)。然后，将 \(x_t\) 和 \(t\) 输入神经网络，让网络预测出我们采样时所用的噪声 \(\epsilon\)。</li>
</ol>

<h2 id="4-严谨的数学证明">4. 严谨的数学证明</h2>

<p>下面我们使用数学归纳法给出完整的数学证明。</p>

<p><strong>定义</strong>:</p>
<ul>
  <li>单步加噪过程: \(x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}\), 其中 \(\alpha_t = 1 - \beta_t\) 且 \(\epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})\)。</li>
  <li>\(\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i\)。</li>
</ul>

<p><strong>目标</strong>:
证明 \(x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon\), 其中 \(\epsilon \sim \mathcal{N}(0, \mathbf{I})\)。</p>

<p><strong>证明过程 (使用数学归纳法)</strong>:</p>

<ol>
  <li>
    <p><strong>基础步骤 (当 \(t=1\))</strong>:
\(x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_0\)
因为 \(\bar{\alpha}_1 = \alpha_1\)，所以公式显然成立。</p>
  </li>
  <li>
    <p><strong>归纳步骤</strong>:
假设公式对 \(t-1\) 成立，即:</p>

\[x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon'\]

    <p>其中 \(\epsilon' \sim \mathcal{N}(0, \mathbf{I})\)。
现在我们来推导 \(t\) 时刻的表达式:</p>

\[x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]

    <p>将 \(x_{t-1}\) 的假设代入:</p>

\[x_t = \sqrt{\alpha_t} (\sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon') + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]

    <p>展开后得到:</p>

\[x_t = \sqrt{\alpha_t \bar{\alpha}_{t-1}} x_0 + \sqrt{\alpha_t(1 - \bar{\alpha}_{t-1})} \epsilon' + \sqrt{1 - \alpha_t} \epsilon_{t-1}\]

    <p>根据定义 \(\bar{\alpha}_t = \alpha_t \bar{\alpha}_{t-1}\)。我们再次合并两个独立的噪声项。合并后噪声项的总方差为:</p>

\[\begin{aligned}
\text{Var}_{\text{total}} &amp;= \left(\sqrt{\alpha_t(1 - \bar{\alpha}_{t-1})}\right)^2 \mathbf{I} + \left(\sqrt{1 - \alpha_t}\right)^2 \mathbf{I} \\
&amp;= (\alpha_t(1 - \bar{\alpha}_{t-1}) + 1 - \alpha_t) \mathbf{I} \\
&amp;= (\alpha_t - \alpha_t\bar{\alpha}_{t-1} + 1 - \alpha_t) \mathbf{I} \\
&amp;= (1 - \alpha_t\bar{\alpha}_{t-1}) \mathbf{I} \\
&amp;= (1 - \bar{\alpha}_t) \mathbf{I}
\end{aligned}\]

    <p>因此，两个噪声项之和等价于一个单一的高斯噪声 \(\sqrt{1 - \bar{\alpha}_t} \epsilon''\)，其中 \(\epsilon'' \sim \mathcal{N}(0, \mathbf{I})\)。最终我们得到:</p>

\[x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon''\]

    <p>公式在 \(t\) 时刻也成立。<strong>证明完毕。</strong></p>
  </li>
</ol>

<h2 id="总结">总结</h2>

<p>DDPM正向过程中的“一步到位”公式，是对分步马尔可夫过程进行递归展开后得到的一个解析解。它是实现高效训练的关键，更揭示了扩散过程的本质——一个由确定性信号和随机性噪声构成的、进程可控的过程。</p>

  </div>
</article>
    </div>
  </main>
  <footer>
    <div class="container">
      <p>&copy; 2025 Chocoleo.</p>
    </div>
  </footer>
</body>
</html>