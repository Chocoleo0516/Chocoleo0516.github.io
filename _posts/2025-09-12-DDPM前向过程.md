---
layout: post
title: "DDPM 正向过程的通项公式"
date: 2025-09-12 21:08:47 +0800
categories: [AI, Diffusion Models]
---

Denoising Diffusion Probabilistic Models (DDPM) 的核心是两个过程：正向加噪和逆向采样过程，本文我们将聚焦正向加噪过程：

论文Background章节的的第二个公式，描述了一个**逐步演化**的过程：

$$
q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})
$$

它又可以写成：

$$
x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon_{t-1}
$$

它告诉我们，任意时刻 $$t$$ 的图像 $$x_t$$，是由其前一时刻的图像 $$x_{t-1}$$ 加上少量噪声得到的。

这是一个典型的马尔可夫过程。

而在正向过程的伪代码中，论文则给出了一个**“一步到位”**的计算方法：

$$
x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon
$$

这个公式表明，我们可以直接从最原始的清晰图像 $$x_0$$，一步跳到任意时刻 $$t$$ 的加噪状态 $$x_t$$。

这也许会让你感到疑惑，**既然扩散过程的理论基础是逐步演化的，为什么我们可以从 $ x\_0 $ 直接推出 $$x_t$$ ？上面两个公式之间究竟是什么关系？**

本文将围绕这个核心问题，深入剖析后一个公式是如何从第一个公式推导而来的，它背后深刻的物理意义，并给出详尽的数学证明。

## 1. 为何需要“一步到位”？
虽然理论上，扩散模型的正向过程（也就是加噪的过程，即训练阶段）是一步一步来的，但是在实际训练时，我们实际上是取某一个时间步的图像进行训练。这个区别关键在于区分理论基础与实践需求。

- **分步定义是理论基石**：正向过程的本质是一个**马尔可夫链**，即当前状态 $ x\_t $ 只依赖于前一个状态 $ x\_\{t-1\} $。这个分步定义 $ q\(x\_t \mid x\_\{t-1\}\) $ 是整个扩散模型的理论核心。它保证了过程的平滑性和数学上的可逆性，这为后续的反向去噪过程（即学习预测 $ p(x_{t-1} \mid x_t) $）提供了坚实的理论依据。

- **直接计算是实践需求**：在训练神经网络时，我们的目标是让模型学会从任意一张加噪图像 $ x\_t $ 中恢复信息（例如，预测所添加的噪声 $$\epsilon$$）。为了高效训练，我们需要快速生成海量的训练数据。如果每次生成 $ x\_t $ 都需要从 $ x\_0 $ 开始迭代 $ t $ 次，当 $ t $ 很大时，这个过程将极其耗时，使得模型训练变得不切实际。

因此，这个“一步到位”的公式正是连接理论与实践的桥梁。它是一个从分步定义中推导出的解析解，让我们能够在常数时间内完成采样，极大地提高了训练效率。

## 2. 公式推导

“一步到位”公式的推导利用了高斯分布的一个性质：**两个独立高斯分布的和仍然是一个高斯分布**。具体来说，如果 $$Z_1 \sim \mathcal{N}(\mu_1, \sigma_1^2\mathbf{I})$$ 和 $$Z_2 \sim \mathcal{N}(\mu_2, \sigma_2^2\mathbf{I})$$ 是独立的，那么它们的和 $$Z_1 + Z_2 \sim \mathcal{N}(\mu_1+\mu_2, (\sigma_1^2 + \sigma_2^2)\mathbf{I})$$。

从单步加噪的公式出发，我们定义 $$\alpha_t = 1 - \beta_t$$，单步过程写作：

$$
x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}, \quad \text{其中 } \epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})
$$

将 $$x_{t-1}$$ 的表达式 $$x_{t-1} = \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}$$ 代入上式：

$$
\begin{aligned}
x_t &= \sqrt{\alpha_t} (\sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon_{t-2}) + \sqrt{1 - \alpha_t} \epsilon_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \underbrace{\sqrt{\alpha_t(1 - \alpha_{t-1})} \epsilon_{t-2} + \sqrt{1 - \alpha_t} \epsilon_{t-1}}_{\text{两个独立高斯噪声的和}}
\end{aligned}
$$

根据高斯分布的可加性，后面两个噪声项可以合并成一个单一的高斯噪声。不断地重复上述过程直到回溯到 $$x_0$$，最终就能得到我们的目标公式。

## 3. 意义
这个公式揭示了正向过程的本质：

1.  任意时刻的加噪图像 $$x_t$$ 都可以看作是**原始图像 $$x_0$$（信号）**和**标准高斯噪声 $$\epsilon$$（噪声）**的一个线性组合。

2.  系数 $$\bar{\alpha}_t = \prod_{i=1}^{t} (1 - \beta_i)$$ 是一个随 $$t$$ 增大而单调递减的数值。它就像一个“进度条”，控制着信号与噪声的平衡：
    - **信号强度**由 $$\sqrt{\bar{\alpha}_t}$$ 控制。当 $$t$$ 很小时，$$\bar{\alpha}_t$$ 接近1，$$x_t$$ 主要由 $$x_0$$ 决定。
    - **噪声强度**由 $$\sqrt{1 - \bar{\alpha}_t}$$ 控制。当 $$t$$ 增大时，$$\bar{\alpha}_t$$ 趋向0，$$x_t$$ 中的噪声成分越来越重。
    - 当 $$t=T$$ 时，$$\bar{\alpha}_T \approx 0$$，此时 $$x_T \approx \epsilon$$。这意味着无论原始图像是什么，最终都会变成一个纯粹的标准高斯噪声。

3.  **训练的基石**：这个公式使得模型的训练目标变得非常明确。给定 $$x_0$$ 和 $$t$$，我们可以通过这个公式生成 $$x_t$$。然后，将 $$x_t$$ 和 $$t$$ 输入神经网络，让网络预测出我们采样时所用的噪声 $$\epsilon$$。

## 4. 严谨的数学证明

下面我们使用数学归纳法给出完整的数学证明。

**定义**:
- 单步加噪过程: $$x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}$$, 其中 $$\alpha_t = 1 - \beta_t$$ 且 $$\epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})$$。
- $$\bar{\alpha}_t = \prod_{i=1}^{t} \alpha_i$$。

**目标**:
证明 $$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$$, 其中 $$\epsilon \sim \mathcal{N}(0, \mathbf{I})$$。

**证明过程 (使用数学归纳法)**:

1.  **基础步骤 (当 $$t=1$$)**:
    $$
    x_1 = \sqrt{\alpha_1} x_0 + \sqrt{1 - \alpha_1} \epsilon_0
    $$
    因为 $$\bar{\alpha}_1 = \alpha_1$$，所以公式显然成立。

2.  **归纳步骤**:
    假设公式对 $$t-1$$ 成立，即:

    $$
    x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon'
    $$

    其中 $$\epsilon' \sim \mathcal{N}(0, \mathbf{I})$$。
    现在我们来推导 $$t$$ 时刻的表达式:

    $$
    x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon_{t-1}
    $$

    将 $$x_{t-1}$$ 的假设代入:

    $$
    x_t = \sqrt{\alpha_t} (\sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon') + \sqrt{1 - \alpha_t} \epsilon_{t-1}
    $$

    展开后得到:

    $$
    x_t = \sqrt{\alpha_t \bar{\alpha}_{t-1}} x_0 + \sqrt{\alpha_t(1 - \bar{\alpha}_{t-1})} \epsilon' + \sqrt{1 - \alpha_t} \epsilon_{t-1}
    $$

    根据定义 $$\bar{\alpha}_t = \alpha_t \bar{\alpha}_{t-1}$$。我们再次合并两个独立的噪声项。合并后噪声项的总方差为:

    $$
    \begin{aligned}
    \text{Var}_{\text{total}} &= \left(\sqrt{\alpha_t(1 - \bar{\alpha}_{t-1})}\right)^2 \mathbf{I} + \left(\sqrt{1 - \alpha_t}\right)^2 \mathbf{I} \\
    &= (\alpha_t(1 - \bar{\alpha}_{t-1}) + 1 - \alpha_t) \mathbf{I} \\
    &= (\alpha_t - \alpha_t\bar{\alpha}_{t-1} + 1 - \alpha_t) \mathbf{I} \\
    &= (1 - \alpha_t\bar{\alpha}_{t-1}) \mathbf{I} \\
    &= (1 - \bar{\alpha}_t) \mathbf{I}
    \end{aligned}
    $$

    因此，两个噪声项之和等价于一个单一的高斯噪声 $$\sqrt{1 - \bar{\alpha}_t} \epsilon''$$，其中 $$\epsilon'' \sim \mathcal{N}(0, \mathbf{I})$$。最终我们得到:

    $$
    x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon''
    $$

    公式在 $$t$$ 时刻也成立。**证明完毕。**

## 总结

DDPM正向过程中的“一步到位”公式，是对分步马尔可夫过程进行递归展开后得到的一个解析解。它是实现高效训练的关键，更揭示了扩散过程的本质——一个由确定性信号和随机性噪声构成的、进程可控的过程。